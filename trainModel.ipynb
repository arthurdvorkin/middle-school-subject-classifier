{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:19:59.786672Z",
     "start_time": "2024-03-24T21:19:59.783553Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-03-28 08:18:51.308298: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-28 08:18:51.350427: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "from subjects import label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:00.402714Z",
     "start_time": "2024-03-24T21:19:59.786865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert/distilbert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=8, id2label=id2label, label2id=label2id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:04.606074Z",
     "start_time": "2024-03-24T21:20:00.404615Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train_dataset_1.csv\", \"validation\": \"validation_dataset_1.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:04.857107Z",
     "start_time": "2024-03-24T21:20:04.723812Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(data_to_tokenize):\n",
    "    text = data_to_tokenize[\"text\"]\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "    \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:05.830729Z",
     "start_time": "2024-03-24T21:20:04.866095Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions,\n",
    "                                          references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:05.988415Z",
     "start_time": "2024-03-24T21:20:05.832746Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:06.027944Z",
     "start_time": "2024-03-24T21:20:06.001504Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification\n",
    "                        r=4, # intrinsic rank of trainable weight matrix\n",
    "                        lora_alpha=32, # this is like a learning rate\n",
    "                        lora_dropout=0.1, # probablity of dropout\n",
    "                        target_modules = ['q_lin']) # we apply lora to query layer only\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.025,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:31:01.386880Z",
     "start_time": "2024-03-24T21:20:06.031314Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35432' max='35432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35432/35432 09:12, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.215787</td>\n",
       "      <td>{'accuracy': 0.9625112917795845}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.207179</td>\n",
       "      <td>{'accuracy': 0.9561878952122854}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.176222</td>\n",
       "      <td>{'accuracy': 0.9715447154471545}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.173681</td>\n",
       "      <td>{'accuracy': 0.9683830171635049}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.165955</td>\n",
       "      <td>{'accuracy': 0.973803071364047}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.139311</td>\n",
       "      <td>{'accuracy': 0.9796747967479674}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.146433</td>\n",
       "      <td>{'accuracy': 0.980126467931346}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.143333</td>\n",
       "      <td>{'accuracy': 0.980126467931346}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.9625112917795845}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9561878952122854}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9715447154471545}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9683830171635049}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.973803071364047}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9796747967479674}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.980126467931346}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.980126467931346}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35432, training_loss=0.16083369660953498, metrics={'train_runtime': 552.9524, 'train_samples_per_second': 256.297, 'train_steps_per_second': 64.078, 'total_flos': 4265303822740800.0, 'train_loss': 0.16083369660953498, 'epoch': 8.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-20.199799 , -20.917608 ,  26.548565 , ..., -17.77113  ,\n",
       "        -23.250957 , -27.736086 ],\n",
       "       [-10.285996 , -13.426138 , -15.403074 , ...,  -5.3500843,\n",
       "          8.068838 ,  -0.2420347],\n",
       "       [-14.182167 , -15.964982 , -16.923204 , ...,  17.386711 ,\n",
       "        -30.5944   , -34.101276 ],\n",
       "       ...,\n",
       "       [-21.258785 , -21.433754 , -24.272427 , ..., -15.490054 ,\n",
       "        -17.568901 , -13.650479 ],\n",
       "       [ 24.024141 , -18.163757 , -34.81647  , ...,  -9.982034 ,\n",
       "        -25.596466 , -18.084906 ],\n",
       "       [-10.436564 ,  -5.757683 , -23.97744  , ..., -11.999895 ,\n",
       "         12.9152565, -11.71101  ]], dtype=float32), label_ids=array([2, 7, 5, ..., 3, 0, 6]), metrics={'test_loss': 0.22537368535995483, 'test_accuracy': {'accuracy': 0.9724604966139955}, 'test_runtime': 3.7449, 'test_samples_per_second': 591.466, 'test_steps_per_second': 147.933})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"csv\", data_files={\"test\":\"test_dataset_1.csv\"})\n",
    "tester_tokenized_dataset = test_dataset.map(tokenize, batched=True)\n",
    "trainer.predict(tester_tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges - Math\n",
      " how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges - Math\n",
      "Who is Alexander the great? - Social Studies\n",
      "Preventing heart attacks can be done by many methods - Health\n",
      "Feet are prone to fungal infections - Science\n",
      "is a student-centered cultural anthropology mini textbook built with an equity lens. We are excited to share this with you all. This book attempts to address the lack of current, reliable, and relevant resources for introductory anthropology courses that center equity and anti-racism. - Social Studies\n",
      "How would you describe ? - Social Studies\n",
      "What is the importance of cells - Science\n",
      "In what year was Armenia invdaded? - Social Studies\n",
      "Matter can change its state under different conditions. We have solids, like ice and rocks, which have a definite shape and volume. Liquids, such as water and juice, take the shape of their container but maintain a constant volume. Gases, like the air we breathe, have neither a definite shape nor volume and fill the space they occupy. - Science\n",
      "The cradle of civilization was in the fertile valleys of rivers like the Tigris and Euphrates in Mesopotamia, the Nile in Egypt, the Indus in the Indian subcontinent, and the Yellow River in China. These early civilizations developed complex societies, writing systems, and technologies that shaped the course of history. - Social Studies\n",
      "feet can get fungal infections if not washed - Health\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "text_list = [\"how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges\",\" how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges\",\n",
    "\"Who is Alexander the great?\", \"Preventing heart attacks can be done by many methods\",\n",
    "\"Feet are prone to fungal infections\", \"is a student-centered cultural anthropology mini textbook built with an equity lens. We are excited to share this with you all. This book attempts to address the lack of current, reliable, and relevant resources for introductory anthropology courses that center equity and anti-racism.\",\n",
    "\"How would you describe ?\", \"What is the importance of cells\", \"In what year was Armenia invdaded?\", \"Matter can change its state under different conditions. We have solids, like ice and rocks, which have a definite shape and volume. Liquids, such as water and juice, take the shape of their container but maintain a constant volume. Gases, like the air we breathe, have neither a definite shape nor volume and fill the space they occupy.\", \"The cradle of civilization was in the fertile valleys of rivers like the Tigris and Euphrates in Mesopotamia, the Nile in Egypt, the Indus in the Indian subcontinent, and the Yellow River in China. These early civilizations developed complex societies, writing systems, and technologies that shaped the course of history.\", \"feet can get fungal infections if not washed\"]\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:365] . invalid file name: ./model/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47953/1055801658.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:365] . invalid file name: ./model/"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "trainer.save_model(\"saved-model1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:19:59.786672Z",
     "start_time": "2024-03-24T21:19:59.783553Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "2024-03-28 08:18:51.308298: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-28 08:18:51.350427: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX512F AVX512_VNNI, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "from subjects import label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:00.402714Z",
     "start_time": "2024-03-24T21:19:59.786865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'distilbert/distilbert-base-uncased'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=8, id2label=id2label, label2id=label2id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:04.606074Z",
     "start_time": "2024-03-24T21:20:00.404615Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"train_dataset_1.csv\", \"validation\": \"validation_dataset_1.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:04.857107Z",
     "start_time": "2024-03-24T21:20:04.723812Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(data_to_tokenize):\n",
    "    text = data_to_tokenize[\"text\"]\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "    \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:05.830729Z",
     "start_time": "2024-03-24T21:20:04.866095Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions,\n",
    "                                          references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:05.988415Z",
     "start_time": "2024-03-24T21:20:05.832746Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:20:06.027944Z",
     "start_time": "2024-03-24T21:20:06.001504Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification\n",
    "                        r=4, # intrinsic rank of trainable weight matrix\n",
    "                        lora_alpha=32, # this is like a learning rate\n",
    "                        lora_dropout=0.1, # probablity of dropout\n",
    "                        target_modules = ['q_lin']) # we apply lora to query layer only\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 8\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"distilbert\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.025,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T21:31:01.386880Z",
     "start_time": "2024-03-24T21:20:06.031314Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35432' max='35432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35432/35432 09:12, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.215787</td>\n",
       "      <td>{'accuracy': 0.9625112917795845}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.207179</td>\n",
       "      <td>{'accuracy': 0.9561878952122854}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.172300</td>\n",
       "      <td>0.176222</td>\n",
       "      <td>{'accuracy': 0.9715447154471545}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.182000</td>\n",
       "      <td>0.173681</td>\n",
       "      <td>{'accuracy': 0.9683830171635049}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.098200</td>\n",
       "      <td>0.165955</td>\n",
       "      <td>{'accuracy': 0.973803071364047}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.139311</td>\n",
       "      <td>{'accuracy': 0.9796747967479674}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.146433</td>\n",
       "      <td>{'accuracy': 0.980126467931346}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.028300</td>\n",
       "      <td>0.143333</td>\n",
       "      <td>{'accuracy': 0.980126467931346}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer is attempting to log a value of \"{'accuracy': 0.9625112917795845}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9561878952122854}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9715447154471545}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9683830171635049}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.973803071364047}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.9796747967479674}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.980126467931346}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "Trainer is attempting to log a value of \"{'accuracy': 0.980126467931346}\" of type <class 'dict'> for key \"eval/accuracy\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35432, training_loss=0.16083369660953498, metrics={'train_runtime': 552.9524, 'train_samples_per_second': 256.297, 'train_steps_per_second': 64.078, 'total_flos': 4265303822740800.0, 'train_loss': 0.16083369660953498, 'epoch': 8.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-20.199799 , -20.917608 ,  26.548565 , ..., -17.77113  ,\n",
       "        -23.250957 , -27.736086 ],\n",
       "       [-10.285996 , -13.426138 , -15.403074 , ...,  -5.3500843,\n",
       "          8.068838 ,  -0.2420347],\n",
       "       [-14.182167 , -15.964982 , -16.923204 , ...,  17.386711 ,\n",
       "        -30.5944   , -34.101276 ],\n",
       "       ...,\n",
       "       [-21.258785 , -21.433754 , -24.272427 , ..., -15.490054 ,\n",
       "        -17.568901 , -13.650479 ],\n",
       "       [ 24.024141 , -18.163757 , -34.81647  , ...,  -9.982034 ,\n",
       "        -25.596466 , -18.084906 ],\n",
       "       [-10.436564 ,  -5.757683 , -23.97744  , ..., -11.999895 ,\n",
       "         12.9152565, -11.71101  ]], dtype=float32), label_ids=array([2, 7, 5, ..., 3, 0, 6]), metrics={'test_loss': 0.22537368535995483, 'test_accuracy': {'accuracy': 0.9724604966139955}, 'test_runtime': 3.7449, 'test_samples_per_second': 591.466, 'test_steps_per_second': 147.933})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = load_dataset(\"csv\", data_files={\"test\":\"test_dataset_1.csv\"})\n",
    "tester_tokenized_dataset = test_dataset.map(tokenize, batched=True)\n",
    "trainer.predict(tester_tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges - Math\n",
      " how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges - Math\n",
      "Who is Alexander the great? - Social Studies\n",
      "Preventing heart attacks can be done by many methods - Health\n",
      "Feet are prone to fungal infections - Science\n",
      "is a student-centered cultural anthropology mini textbook built with an equity lens. We are excited to share this with you all. This book attempts to address the lack of current, reliable, and relevant resources for introductory anthropology courses that center equity and anti-racism. - Social Studies\n",
      "How would you describe ? - Social Studies\n",
      "What is the importance of cells - Science\n",
      "In what year was Armenia invdaded? - Social Studies\n",
      "Matter can change its state under different conditions. We have solids, like ice and rocks, which have a definite shape and volume. Liquids, such as water and juice, take the shape of their container but maintain a constant volume. Gases, like the air we breathe, have neither a definite shape nor volume and fill the space they occupy. - Science\n",
      "The cradle of civilization was in the fertile valleys of rivers like the Tigris and Euphrates in Mesopotamia, the Nile in Egypt, the Indus in the Indian subcontinent, and the Yellow River in China. These early civilizations developed complex societies, writing systems, and technologies that shaped the course of history. - Social Studies\n",
      "feet can get fungal infections if not washed - Health\n"
     ]
    }
   ],
   "source": [
    "model.to('cpu')\n",
    "text_list = [\"how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges\",\" how much space the oranges need by themselves, what is  the remaining space, determine how many equal spaces you have between the oranges\",\n",
    "\"Who is Alexander the great?\", \"Preventing heart attacks can be done by many methods\",\n",
    "\"Feet are prone to fungal infections\", \"is a student-centered cultural anthropology mini textbook built with an equity lens. We are excited to share this with you all. This book attempts to address the lack of current, reliable, and relevant resources for introductory anthropology courses that center equity and anti-racism.\",\n",
    "\"How would you describe ?\", \"What is the importance of cells\", \"In what year was Armenia invdaded?\", \"Matter can change its state under different conditions. We have solids, like ice and rocks, which have a definite shape and volume. Liquids, such as water and juice, take the shape of their container but maintain a constant volume. Gases, like the air we breathe, have neither a definite shape nor volume and fill the space they occupy.\", \"The cradle of civilization was in the fertile valleys of rivers like the Tigris and Euphrates in Mesopotamia, the Nile in Egypt, the Indus in the Indian subcontinent, and the Yellow River in China. These early civilizations developed complex societies, writing systems, and technologies that shaped the course of history.\", \"feet can get fungal infections if not washed\"]\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in text_list:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:365] . invalid file name: ./model/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47953/1055801658.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./model/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:365] . invalid file name: ./model/"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "trainer.save_model(\"saved-model1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
